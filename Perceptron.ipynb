{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d55cdd24-9990-4922-b545-fd0dd1cd8e00",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80c66dc2-d651-4465-9654-4ba5ac5e9244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Embedding, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "from statistics import mean\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027afa11-eca7-4583-a371-d1019a994d66",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a16df25e-24a7-4438-bce8-7f69ec89ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_processed_text = 'gemini_embedding'\n",
    "# pre_processed_text = 'text_embed'\n",
    "pre_processed_text = 'pre_processed_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89a1d48e-5f34-497b-a9f0-572119b12a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./dataset/hsd_pre_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a10b906-55de-4a1c-b5ed-8ee0d6586997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hatespeech_comb</th>\n",
       "      <th>hatespeech_G1</th>\n",
       "      <th>annotator_G1</th>\n",
       "      <th>hatespeech_G2</th>\n",
       "      <th>annotator_G2</th>\n",
       "      <th>hatespeech_G3</th>\n",
       "      <th>annotator_G3</th>\n",
       "      <th>pre_processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@__andrea__b \\nO cara vive em outro mundo\\nNão...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>V</td>\n",
       "      <td>0</td>\n",
       "      <td>E</td>\n",
       "      <td>cara vive outro mundo mundo real refugiados vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@_carmeloneto Estes incompetentes não cuidam n...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>0.0</td>\n",
       "      <td>V</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>incompetentes cuidam povo brasileiro poucos re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@_carmeloneto \\nOs 'cumpanhero' quebraram toda...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "      <td>E</td>\n",
       "      <td>cumpanhero quebraram toda regras</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@_GlitteryKisses é isso não conseguem pensar n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>0.0</td>\n",
       "      <td>V</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "      <td>conseguem pensar sentido lato além vê frente o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@_iglira bom dia macaco branco haha</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>bom dia macaco branco haha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5665</th>\n",
       "      <td>@zecarlosantos2 é o unico que nao se corrompe....</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>unico nao corrompenao vende chega aroporto apl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5666</th>\n",
       "      <td>@zqkitowz sei das cotas, mas não sabia disso, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>sei cotas sabia disso putaria porra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5667</th>\n",
       "      <td>@zqkitowz sim, a maioria do eleitorado é mulhe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>0.0</td>\n",
       "      <td>V</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>sim maioria eleitorado mulher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5668</th>\n",
       "      <td>@zurcju seguir no tt é facíl, apresentar as am...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>seguir tt facíl apresentar amigas sapatão ngm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5669</th>\n",
       "      <td>na vdd a culpa é do menino de 11 anos otário q...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "      <td>vdd culpa menino ano otário chamava burragorda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5670 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  hatespeech_comb  \\\n",
       "0     @__andrea__b \\nO cara vive em outro mundo\\nNão...                1   \n",
       "1     @_carmeloneto Estes incompetentes não cuidam n...                0   \n",
       "2     @_carmeloneto \\nOs 'cumpanhero' quebraram toda...                0   \n",
       "3     @_GlitteryKisses é isso não conseguem pensar n...                0   \n",
       "4                   @_iglira bom dia macaco branco haha                1   \n",
       "...                                                 ...              ...   \n",
       "5665  @zecarlosantos2 é o unico que nao se corrompe....                0   \n",
       "5666  @zqkitowz sei das cotas, mas não sabia disso, ...                1   \n",
       "5667  @zqkitowz sim, a maioria do eleitorado é mulhe...                0   \n",
       "5668  @zurcju seguir no tt é facíl, apresentar as am...                1   \n",
       "5669  na vdd a culpa é do menino de 11 anos otário q...                1   \n",
       "\n",
       "      hatespeech_G1 annotator_G1  hatespeech_G2 annotator_G2  hatespeech_G3  \\\n",
       "0                 1            A            1.0            V              0   \n",
       "1                 1            D            0.0            V              0   \n",
       "2                 1            A            0.0            B              0   \n",
       "3                 0            C            0.0            V              0   \n",
       "4                 0            A            1.0            I              1   \n",
       "...             ...          ...            ...          ...            ...   \n",
       "5665              1            C            0.0            B              0   \n",
       "5666              1            D            1.0           It              0   \n",
       "5667              0            C            0.0            V              0   \n",
       "5668              1            C            1.0            S              0   \n",
       "5669              1            E            1.0           It              0   \n",
       "\n",
       "     annotator_G3                                 pre_processed_text  \n",
       "0               E  cara vive outro mundo mundo real refugiados vi...  \n",
       "1               C  incompetentes cuidam povo brasileiro poucos re...  \n",
       "2               E                   cumpanhero quebraram toda regras  \n",
       "3               D  conseguem pensar sentido lato além vê frente o...  \n",
       "4               E                         bom dia macaco branco haha  \n",
       "...           ...                                                ...  \n",
       "5665            A  unico nao corrompenao vende chega aroporto apl...  \n",
       "5666            A                sei cotas sabia disso putaria porra  \n",
       "5667            C                      sim maioria eleitorado mulher  \n",
       "5668            A  seguir tt facíl apresentar amigas sapatão ngm ...  \n",
       "5669            D  vdd culpa menino ano otário chamava burragorda...  \n",
       "\n",
       "[5670 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c238de8c-97dc-476b-801b-a2f7ce683a81",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3aa301-c558-42a4-93a7-6ff08ffb977d",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df1954e3-1ae2-47a7-82a4-5b3e798309c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1193514it [00:11, 102568.63it/s]\n"
     ]
    }
   ],
   "source": [
    "GLOVE_MODEL_FILE = './dataset/glove.twitter.27B/glove.twitter.27B.100d.txt'\n",
    "max_len = 128\n",
    "embedding_dim = 100\n",
    "\n",
    "# Tokenize\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(df['pre_processed_text'])\n",
    "seq = token.texts_to_sequences(df['pre_processed_text'])\n",
    "\n",
    "# Padding\n",
    "pad_seq = pad_sequences(seq,maxlen=embedding_dim)\n",
    "\n",
    "# Vocab size\n",
    "vocab_size = len(token.word_index)+1\n",
    "\n",
    "# Load embedding vector\n",
    "embedding_vector = {}\n",
    "f = open(GLOVE_MODEL_FILE)\n",
    "for line in tqdm(f):\n",
    "    value = line.split(' ')\n",
    "    word = value[0]\n",
    "    coef = np.array(value[1:],dtype = 'float32')\n",
    "    embedding_vector[word] = coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "686e4ff6-37e9-4891-8d50-4fc302efd7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15340/15340 [00:00<00:00, 528239.47it/s]\n"
     ]
    }
   ],
   "source": [
    "# Keep a out of vocabullary dict\n",
    "oov_dict = {}\n",
    "\n",
    "# Generate embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size,embedding_dim))\n",
    "for word,i in tqdm(token.word_index.items()):\n",
    "    embedding_value = embedding_vector.get(word)\n",
    "    if embedding_value is not None:\n",
    "        embedding_matrix[i] = embedding_value\n",
    "    else:\n",
    "        oov_dict[word] = np.random.uniform(-1., 1., (embedding_dim,)) # Generate new random vector\n",
    "        embedding_matrix[i] = oov_dict[word]\n",
    "\n",
    "\n",
    "# Transform text into embed vector\n",
    "embedded_sequences = np.zeros((len(pad_seq), max_len, embedding_dim))\n",
    "for i, seq in enumerate(pad_seq):\n",
    "    for j, idx in enumerate(seq):\n",
    "        if idx > 0:  # Skip padding index\n",
    "            embedded_sequences[i, j] = embedding_matrix[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9920db4a-b921-4306-900d-1e1121031ea6",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0750e4c5-367f-40c7-9cac-74c761034890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(X_train, X_test, n_grams):\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, n_grams))\n",
    "    X_train = vectorizer.fit_transform(X_train).toarray()\n",
    "    X_test = vectorizer.transform(X_test).toarray()\n",
    "    return X_train,X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac7dd0b-303b-438a-b90d-83a419a0f806",
   "metadata": {},
   "source": [
    "# Split into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1591290-f83d-4841-8f5a-d52c8be845a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df[pre_processed_text]\n",
    "X = embedded_sequences\n",
    "y = df['hatespeech_comb']\n",
    "\n",
    "# Flatten\n",
    "X = np.array([matrix.ravel() for matrix in X])\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Hold out\n",
    "sss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, train_size = 0.8, random_state = RANDOM_STATE)\n",
    "for i, (train_index, test_index) in enumerate(sss.split(X, y)):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b092b63-c48c-411e-a370-aa8c0f7eb306",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e829dbc6-3734-49d5-998c-56fb1d5dd0a1",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74cc5292-47a4-40bf-bca9-49a52c4fe32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_func = 'relu'\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "def mlp(learning_rate=0.001, activation_func='relu', clipvalue=0.5):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(shape=(max_len*embedding_dim,)))\n",
    "    model.add(Dense(units = 1000, activation = activation_func, kernel_initializer = 'random_uniform'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units = 500, activation = activation_func, kernel_initializer = 'random_uniform'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "    \n",
    "    adam = Adam(learning_rate = learning_rate, clipvalue = clipvalue)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['binary_accuracy'])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07b34e80-8b39-44a9-a173-9433efee196e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training\n",
      "Folder :0\n",
      "Epoch 1/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - binary_accuracy: 0.6653 - loss: 0.6348\n",
      "Epoch 2/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.7892 - loss: 0.4351\n",
      "Epoch 3/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.9257 - loss: 0.2389\n",
      "Epoch 4/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.9715 - loss: 0.0983\n",
      "Epoch 5/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.9873 - loss: 0.0498\n",
      "Epoch 6/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.9958 - loss: 0.0259\n",
      "Epoch 7/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.9985 - loss: 0.0107\n",
      "Epoch 8/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.9980 - loss: 0.0086\n",
      "Epoch 9/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.9981 - loss: 0.0092\n",
      "Epoch 10/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.9990 - loss: 0.0095\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "# F1: 0.5299806576402321\n",
      "# Accuracy: 0.7323788546255506\n",
      "===============\n",
      "Folder :1\n",
      "Epoch 1/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - binary_accuracy: 0.6633 - loss: 0.6584\n",
      "Epoch 2/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - binary_accuracy: 0.8103 - loss: 0.4291\n",
      "Epoch 3/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.9277 - loss: 0.2262\n",
      "Epoch 4/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - binary_accuracy: 0.9702 - loss: 0.0961\n",
      "Epoch 5/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - binary_accuracy: 0.9869 - loss: 0.0487\n",
      "Epoch 6/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - binary_accuracy: 0.9951 - loss: 0.0220\n",
      "Epoch 7/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.9979 - loss: 0.0148\n",
      "Epoch 8/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - binary_accuracy: 0.9974 - loss: 0.0122\n",
      "Epoch 9/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.9982 - loss: 0.0094\n",
      "Epoch 10/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - binary_accuracy: 0.9992 - loss: 0.0049\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "# F1: 0.5074626865671642\n",
      "# Accuracy: 0.7092511013215859\n",
      "===============\n",
      "Folder :2\n",
      "Epoch 1/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.6440 - loss: 0.6770\n",
      "Epoch 2/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.8167 - loss: 0.4064\n",
      "Epoch 3/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.9165 - loss: 0.2259\n",
      "Epoch 4/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - binary_accuracy: 0.9766 - loss: 0.0938\n",
      "Epoch 5/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.9930 - loss: 0.0299\n",
      "Epoch 6/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.9945 - loss: 0.0260\n",
      "Epoch 7/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.9948 - loss: 0.0204\n",
      "Epoch 8/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.9969 - loss: 0.0169\n",
      "Epoch 9/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.9967 - loss: 0.0120\n",
      "Epoch 10/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.9987 - loss: 0.0060\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x473d1e700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "# F1: 0.4794007490636704\n",
      "# Accuracy: 0.6938325991189427\n",
      "===============\n",
      "Folder :3\n",
      "Epoch 1/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - binary_accuracy: 0.6855 - loss: 0.6380\n",
      "Epoch 2/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - binary_accuracy: 0.8216 - loss: 0.4026\n",
      "Epoch 3/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - binary_accuracy: 0.9046 - loss: 0.2329\n",
      "Epoch 4/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - binary_accuracy: 0.9843 - loss: 0.0778\n",
      "Epoch 5/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.9928 - loss: 0.0274\n",
      "Epoch 6/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - binary_accuracy: 0.9942 - loss: 0.0359\n",
      "Epoch 7/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - binary_accuracy: 0.9967 - loss: 0.0159\n",
      "Epoch 8/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - binary_accuracy: 0.9965 - loss: 0.0133\n",
      "Epoch 9/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - binary_accuracy: 0.9984 - loss: 0.0079\n",
      "Epoch 10/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - binary_accuracy: 0.9987 - loss: 0.0049\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x473fa65c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "# F1: 0.46732673267326735\n",
      "# Accuracy: 0.7037444933920705\n",
      "===============\n",
      "Folder :4\n",
      "Epoch 1/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - binary_accuracy: 0.6159 - loss: 0.7244\n",
      "Epoch 2/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - binary_accuracy: 0.7428 - loss: 0.4897\n",
      "Epoch 3/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - binary_accuracy: 0.8746 - loss: 0.3123\n",
      "Epoch 4/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.9583 - loss: 0.1336\n",
      "Epoch 5/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.9882 - loss: 0.0444\n",
      "Epoch 6/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.9933 - loss: 0.0248\n",
      "Epoch 7/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - binary_accuracy: 0.9964 - loss: 0.0245\n",
      "Epoch 8/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - binary_accuracy: 0.9973 - loss: 0.0130\n",
      "Epoch 9/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.9969 - loss: 0.0117\n",
      "Epoch 10/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.9983 - loss: 0.0056\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "# F1: 0.4532803180914513\n",
      "# Accuracy: 0.697136563876652\n",
      "===============\n",
      "# Mean Accuracy:  0.7072687224669604\n",
      "# Mean F1:  0.4874902288071571\n"
     ]
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, train_size=0.8, random_state=42)\n",
    "\n",
    "results = []\n",
    "f1 = []\n",
    "accuracy = []\n",
    "\n",
    "X = embedded_sequences\n",
    "y = df['hatespeech_comb']\n",
    "\n",
    "X = np.array([matrix.ravel() for matrix in X])\n",
    "\n",
    "print(\"# Training\")\n",
    "\n",
    "for i, (train_index_cv, val_index) in enumerate(sss.split(X_train, y_train)):\n",
    "    print(f\"Folder :{i}\")\n",
    "    X_train_cv, X_val = X[train_index_cv], X[val_index]\n",
    "    y_train_cv, y_val = y[train_index_cv], y[val_index]\n",
    "\n",
    "    \n",
    "    # Define EarlyStopping callback\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    # Model\n",
    "    model = KerasClassifier(model=mlp,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping])\n",
    "    \n",
    "    # Fit\n",
    "    model.fit(X_train_cv, y_train_cv)\n",
    "    pred = model.predict(X_val)\n",
    "\n",
    "    result = classification_report(y_val, pred)\n",
    "    results.append(result)\n",
    "\n",
    "    f = f1_score(y_val, pred)\n",
    "    f1.append(f)\n",
    "    print(f\"# F1: {f}\")\n",
    "\n",
    "    acc = accuracy_score(y_val, pred)\n",
    "    accuracy.append(acc)\n",
    "    print(f\"# Accuracy: {acc}\")\n",
    "    print(\"===============\")\n",
    "\n",
    "print(\"# Mean Accuracy: \", mean(accuracy))\n",
    "print(\"# Mean F1: \", mean(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bea02d-2cc3-4d55-8bb4-3534f9e84ede",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d64a32fd-d07d-4672-b094-8de06b7fbaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - binary_accuracy: 0.6369 - loss: 0.6837\n",
      "Epoch 2/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - binary_accuracy: 0.7710 - loss: 0.4622\n",
      "Epoch 3/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.8902 - loss: 0.2815\n",
      "Epoch 4/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - binary_accuracy: 0.9657 - loss: 0.1231\n",
      "Epoch 5/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - binary_accuracy: 0.9843 - loss: 0.0604\n",
      "Epoch 6/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - binary_accuracy: 0.9941 - loss: 0.0283\n",
      "Epoch 7/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - binary_accuracy: 0.9950 - loss: 0.0207\n",
      "Epoch 8/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.9948 - loss: 0.0261\n",
      "Epoch 9/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.9980 - loss: 0.0140\n",
      "Epoch 10/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - binary_accuracy: 0.9970 - loss: 0.0144\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.82      0.79       776\n",
      "           1       0.54      0.47      0.50       358\n",
      "\n",
      "    accuracy                           0.71      1134\n",
      "   macro avg       0.66      0.64      0.65      1134\n",
      "weighted avg       0.70      0.71      0.70      1134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(model = mlp,\n",
    "                        epochs = epochs,\n",
    "                        batch_size = batch_size,\n",
    "                        callbacks=[early_stopping])\n",
    "\n",
    "model.fit(pd.DataFrame(X_train.tolist()), y_train)\n",
    "pred = model.predict(pd.DataFrame(X_test.tolist()))\n",
    "\n",
    "result = classification_report(y_test, pred)\n",
    "f1 = f1_score(y_test, pred)\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
