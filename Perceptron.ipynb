{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d55cdd24-9990-4922-b545-fd0dd1cd8e00",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80c66dc2-d651-4465-9654-4ba5ac5e9244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import InputLayer, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027afa11-eca7-4583-a371-d1019a994d66",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89a1d48e-5f34-497b-a9f0-572119b12a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./dataset/hsd_pre_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a10b906-55de-4a1c-b5ed-8ee0d6586997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>hatespeech_comb</th>\n",
       "      <th>hatespeech_G1</th>\n",
       "      <th>annotator_G1</th>\n",
       "      <th>hatespeech_G2</th>\n",
       "      <th>annotator_G2</th>\n",
       "      <th>hatespeech_G3</th>\n",
       "      <th>annotator_G3</th>\n",
       "      <th>pre_processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@__andrea__b \\nO cara vive em outro mundo\\nNão...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>V</td>\n",
       "      <td>0</td>\n",
       "      <td>E</td>\n",
       "      <td>cara vive outro mundo mundo real refugiados vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@_carmeloneto Estes incompetentes não cuidam n...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>0.0</td>\n",
       "      <td>V</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>incompetentes cuidam povo brasileiro poucos re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>@_carmeloneto \\nOs 'cumpanhero' quebraram toda...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "      <td>E</td>\n",
       "      <td>cumpanhero quebraram toda regras</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>@_GlitteryKisses é isso não conseguem pensar n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>0.0</td>\n",
       "      <td>V</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "      <td>conseguem pensar sentido lato além vê frente o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@_iglira bom dia macaco branco haha</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>bom dia macaco branco haha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5665</th>\n",
       "      <td>5665</td>\n",
       "      <td>@zecarlosantos2 é o unico que nao se corrompe....</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>unico nao corrompenao vende chega aroporto apl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5666</th>\n",
       "      <td>5666</td>\n",
       "      <td>@zqkitowz sei das cotas, mas não sabia disso, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>sei cotas sabia disso putaria porra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5667</th>\n",
       "      <td>5667</td>\n",
       "      <td>@zqkitowz sim, a maioria do eleitorado é mulhe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>0.0</td>\n",
       "      <td>V</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>sim maioria eleitorado mulher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5668</th>\n",
       "      <td>5668</td>\n",
       "      <td>@zurcju seguir no tt é facíl, apresentar as am...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>1.0</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>seguir tt facíl apresentar amigas sapatão ngm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5669</th>\n",
       "      <td>5669</td>\n",
       "      <td>na vdd a culpa é do menino de 11 anos otário q...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "      <td>vdd culpa menino ano otário chamava burragorda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5670 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               text  \\\n",
       "0              0  @__andrea__b \\nO cara vive em outro mundo\\nNão...   \n",
       "1              1  @_carmeloneto Estes incompetentes não cuidam n...   \n",
       "2              2  @_carmeloneto \\nOs 'cumpanhero' quebraram toda...   \n",
       "3              3  @_GlitteryKisses é isso não conseguem pensar n...   \n",
       "4              4                @_iglira bom dia macaco branco haha   \n",
       "...          ...                                                ...   \n",
       "5665        5665  @zecarlosantos2 é o unico que nao se corrompe....   \n",
       "5666        5666  @zqkitowz sei das cotas, mas não sabia disso, ...   \n",
       "5667        5667  @zqkitowz sim, a maioria do eleitorado é mulhe...   \n",
       "5668        5668  @zurcju seguir no tt é facíl, apresentar as am...   \n",
       "5669        5669  na vdd a culpa é do menino de 11 anos otário q...   \n",
       "\n",
       "      hatespeech_comb  hatespeech_G1 annotator_G1  hatespeech_G2 annotator_G2  \\\n",
       "0                   1              1            A            1.0            V   \n",
       "1                   0              1            D            0.0            V   \n",
       "2                   0              1            A            0.0            B   \n",
       "3                   0              0            C            0.0            V   \n",
       "4                   1              0            A            1.0            I   \n",
       "...               ...            ...          ...            ...          ...   \n",
       "5665                0              1            C            0.0            B   \n",
       "5666                1              1            D            1.0           It   \n",
       "5667                0              0            C            0.0            V   \n",
       "5668                1              1            C            1.0            S   \n",
       "5669                1              1            E            1.0           It   \n",
       "\n",
       "      hatespeech_G3 annotator_G3  \\\n",
       "0                 0            E   \n",
       "1                 0            C   \n",
       "2                 0            E   \n",
       "3                 0            D   \n",
       "4                 1            E   \n",
       "...             ...          ...   \n",
       "5665              0            A   \n",
       "5666              0            A   \n",
       "5667              0            C   \n",
       "5668              0            A   \n",
       "5669              0            D   \n",
       "\n",
       "                                     pre_processed_text  \n",
       "0     cara vive outro mundo mundo real refugiados vi...  \n",
       "1     incompetentes cuidam povo brasileiro poucos re...  \n",
       "2                      cumpanhero quebraram toda regras  \n",
       "3     conseguem pensar sentido lato além vê frente o...  \n",
       "4                            bom dia macaco branco haha  \n",
       "...                                                 ...  \n",
       "5665  unico nao corrompenao vende chega aroporto apl...  \n",
       "5666                sei cotas sabia disso putaria porra  \n",
       "5667                      sim maioria eleitorado mulher  \n",
       "5668  seguir tt facíl apresentar amigas sapatão ngm ...  \n",
       "5669  vdd culpa menino ano otário chamava burragorda...  \n",
       "\n",
       "[5670 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c238de8c-97dc-476b-801b-a2f7ce683a81",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3aa301-c558-42a4-93a7-6ff08ffb977d",
   "metadata": {},
   "source": [
    "## CHI-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df1954e3-1ae2-47a7-82a4-5b3e798309c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['pre_processed_text']\n",
    "y = df['hatespeech_comb']\n",
    "\n",
    "\n",
    "def feature_selection_chi2(X,y):\n",
    "  normalizer = MinMaxScaler()\n",
    "  X_norm = normalizer.fit_transform(X)\n",
    "  chi_selector = SelectKBest(chi2, k=241)\n",
    "  chi_selector.fit(X_norm, y)\n",
    "\n",
    "  chi_support = chi_selector.get_support()\n",
    "  selected_features = np.where(chi_support)[0]\n",
    "  #chi_feature = X.loc[:,chi_support].columns.tolist()\n",
    "  #print(str(len(selected_features)), 'selected features')\n",
    "  return selected_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9920db4a-b921-4306-900d-1e1121031ea6",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0750e4c5-367f-40c7-9cac-74c761034890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(X_train, X_test, n_grams):\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, n_grams))\n",
    "    X_train = vectorizer.fit_transform(X_train).toarray()\n",
    "    X_test = vectorizer.transform(X_test).toarray()\n",
    "    return X_train,X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac7dd0b-303b-438a-b90d-83a419a0f806",
   "metadata": {},
   "source": [
    "# Split into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1591290-f83d-4841-8f5a-d52c8be845a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['pre_processed_text']\n",
    "y = df['hatespeech_comb']\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Hold out\n",
    "sss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, train_size = 0.8, random_state = RANDOM_STATE)\n",
    "for i, (train_index, test_index) in enumerate(sss.split(X, y)):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b092b63-c48c-411e-a370-aa8c0f7eb306",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e829dbc6-3734-49d5-998c-56fb1d5dd0a1",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74cc5292-47a4-40bf-bca9-49a52c4fe32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_func = 'relu'\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "max_len = 50000\n",
    "\n",
    "def mlp():\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(shape=(max_len,)))\n",
    "    model.add(Dense(units = 100, activation = activation_func, kernel_initializer = 'random_uniform'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units = 50, activation = activation_func, kernel_initializer = 'random_uniform'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "    \n",
    "    adam = Adam(learning_rate = learning_rate, clipvalue = 0.5)\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['binary_accuracy'])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5623017-0049-43ee-bb84-1d61f14b41ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training\n",
      "Folder :0\n",
      "Epoch 1/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - binary_accuracy: 0.6872 - loss: 0.6624\n",
      "Epoch 2/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - binary_accuracy: 0.6853 - loss: 0.5199\n",
      "Epoch 3/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - binary_accuracy: 0.7419 - loss: 0.3389\n",
      "Epoch 4/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - binary_accuracy: 0.9878 - loss: 0.1964\n",
      "Epoch 5/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - binary_accuracy: 0.9966 - loss: 0.0700\n",
      "Epoch 6/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - binary_accuracy: 0.9962 - loss: 0.0263\n",
      "Epoch 7/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - binary_accuracy: 0.9981 - loss: 0.0107\n",
      "Epoch 8/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - binary_accuracy: 0.9986 - loss: 0.0117\n",
      "Epoch 9/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - binary_accuracy: 0.9989 - loss: 0.0051\n",
      "Epoch 10/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - binary_accuracy: 0.9993 - loss: 0.0033\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "# F1: 0.5507246376811594\n",
      "# Accuracy: 0.7268722466960352\n",
      "===============\n",
      "Folder :1\n",
      "Epoch 1/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - binary_accuracy: 0.6775 - loss: 0.6628\n",
      "Epoch 2/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - binary_accuracy: 0.6809 - loss: 0.5231\n",
      "Epoch 3/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - binary_accuracy: 0.6995 - loss: 0.3591\n",
      "Epoch 4/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - binary_accuracy: 0.9659 - loss: 0.2331\n",
      "Epoch 5/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - binary_accuracy: 0.9960 - loss: 0.1169\n",
      "Epoch 6/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - binary_accuracy: 0.9952 - loss: 0.0329\n",
      "Epoch 7/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - binary_accuracy: 0.9956 - loss: 0.0143\n",
      "Epoch 8/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - binary_accuracy: 0.9977 - loss: 0.0080\n",
      "Epoch 9/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - binary_accuracy: 0.9994 - loss: 0.0049\n",
      "Epoch 10/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - binary_accuracy: 0.9983 - loss: 0.0054\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "# F1: 0.5669014084507042\n",
      "# Accuracy: 0.7290748898678414\n",
      "===============\n",
      "Folder :2\n",
      "Epoch 1/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - binary_accuracy: 0.6429 - loss: 0.6712\n",
      "Epoch 2/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - binary_accuracy: 0.6944 - loss: 0.5326\n",
      "Epoch 3/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - binary_accuracy: 0.7012 - loss: 0.3672\n",
      "Epoch 4/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - binary_accuracy: 0.9060 - loss: 0.2482\n",
      "Epoch 5/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - binary_accuracy: 0.9924 - loss: 0.1533\n",
      "Epoch 6/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - binary_accuracy: 0.9966 - loss: 0.0767\n",
      "Epoch 7/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - binary_accuracy: 0.9975 - loss: 0.0239\n",
      "Epoch 8/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - binary_accuracy: 0.9984 - loss: 0.0117\n",
      "Epoch 9/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - binary_accuracy: 0.9983 - loss: 0.0068\n",
      "Epoch 10/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - binary_accuracy: 0.9980 - loss: 0.0061\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x13ff08720> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "# F1: 0.565359477124183\n",
      "# Accuracy: 0.7070484581497798\n",
      "===============\n",
      "Folder :3\n",
      "Epoch 1/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - binary_accuracy: 0.6665 - loss: 0.6646\n",
      "Epoch 2/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - binary_accuracy: 0.6877 - loss: 0.5206\n",
      "Epoch 3/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - binary_accuracy: 0.7048 - loss: 0.3503\n",
      "Epoch 4/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - binary_accuracy: 0.9657 - loss: 0.2362\n",
      "Epoch 5/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - binary_accuracy: 0.9950 - loss: 0.1326\n",
      "Epoch 6/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - binary_accuracy: 0.9958 - loss: 0.0444\n",
      "Epoch 7/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - binary_accuracy: 0.9984 - loss: 0.0159\n",
      "Epoch 8/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - binary_accuracy: 0.9985 - loss: 0.0105\n",
      "Epoch 9/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - binary_accuracy: 0.9977 - loss: 0.0077\n",
      "Epoch 10/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - binary_accuracy: 0.9991 - loss: 0.0046\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x16c8df420> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "# F1: 0.5585284280936454\n",
      "# Accuracy: 0.7092511013215859\n",
      "===============\n",
      "Folder :4\n",
      "Epoch 1/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - binary_accuracy: 0.6716 - loss: 0.6672\n",
      "Epoch 2/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - binary_accuracy: 0.6792 - loss: 0.5315\n",
      "Epoch 3/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - binary_accuracy: 0.7415 - loss: 0.3497\n",
      "Epoch 4/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - binary_accuracy: 0.9805 - loss: 0.1966\n",
      "Epoch 5/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - binary_accuracy: 0.9948 - loss: 0.0762\n",
      "Epoch 6/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - binary_accuracy: 0.9987 - loss: 0.0230\n",
      "Epoch 7/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - binary_accuracy: 0.9990 - loss: 0.0100\n",
      "Epoch 8/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - binary_accuracy: 0.9988 - loss: 0.0066\n",
      "Epoch 9/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - binary_accuracy: 0.9991 - loss: 0.0035\n",
      "Epoch 10/10\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - binary_accuracy: 0.9993 - loss: 0.0030\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "# F1: 0.5061511423550088\n",
      "# Accuracy: 0.6905286343612335\n",
      "===============\n",
      "# Mean Accuracy:  0.7125550660792952\n",
      "# Mean F1:  0.5495330187409402\n"
     ]
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=5, test_size = 0.2, train_size = 0.8, random_state=42)\n",
    "\n",
    "results = []\n",
    "f1 = []\n",
    "precision = []\n",
    "recall = []\n",
    "accuracy = []\n",
    "\n",
    "X = df['pre_processed_text']\n",
    "y = df['hatespeech_comb']\n",
    "n_gram = 2\n",
    "\n",
    "\n",
    "print(\"# Training\")\n",
    "\n",
    "\n",
    "for i, (train_index_cv, val_index) in enumerate(sss.split(X_train, y_train)):\n",
    "    print(f\"Folder :{i}\")\n",
    "    X_train_cv, X_val = X[train_index_cv], X[val_index]\n",
    "    y_train_cv, y_val = y[train_index_cv], y[val_index]\n",
    "\n",
    "    # bag of words\n",
    "    X_train_cv, X_val = bag_of_words(X_train_cv, X_val, n_gram)\n",
    "\n",
    "    # Padding\n",
    "    X_train_cv = pad_sequences(X_train_cv, maxlen=max_len, padding='post')\n",
    "    X_val = pad_sequences(X_val, maxlen=max_len, padding='post')\n",
    "\n",
    "    # Model\n",
    "    model = KerasClassifier(model=mlp,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size)\n",
    "    \n",
    "    # Fit\n",
    "    model.fit(X_train_cv, y_train_cv)\n",
    "    pred = model.predict(X_val)\n",
    "\n",
    "    result = classification_report(y_val, pred)\n",
    "    results.append(result)\n",
    "\n",
    "    f = f1_score(y_val, pred)\n",
    "    f1.append(f)\n",
    "    print(f\"# F1: {f}\")\n",
    "\n",
    "    acc = accuracy_score(y_val, pred)\n",
    "    accuracy.append(acc)\n",
    "    print(f\"# Accuracy: {acc}\")\n",
    "    print(\"===============\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"# Mean Accuracy: \", mean(accuracy))\n",
    "print(\"# Mean F1: \", mean(f1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bea02d-2cc3-4d55-8bb4-3534f9e84ede",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d64a32fd-d07d-4672-b094-8de06b7fbaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - binary_accuracy: 0.6437 - loss: 0.6659\n",
      "Epoch 2/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - binary_accuracy: 0.6770 - loss: 0.5188\n",
      "Epoch 3/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - binary_accuracy: 0.7753 - loss: 0.3311\n",
      "Epoch 4/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - binary_accuracy: 0.9883 - loss: 0.1638\n",
      "Epoch 5/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - binary_accuracy: 0.9925 - loss: 0.0440\n",
      "Epoch 6/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - binary_accuracy: 0.9964 - loss: 0.0158\n",
      "Epoch 7/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - binary_accuracy: 0.9971 - loss: 0.0113\n",
      "Epoch 8/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - binary_accuracy: 0.9991 - loss: 0.0068\n",
      "Epoch 9/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - binary_accuracy: 0.9988 - loss: 0.0054\n",
      "Epoch 10/10\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - binary_accuracy: 0.9988 - loss: 0.0044\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.80      0.80       776\n",
      "           1       0.57      0.58      0.58       358\n",
      "\n",
      "    accuracy                           0.73      1134\n",
      "   macro avg       0.69      0.69      0.69      1134\n",
      "weighted avg       0.73      0.73      0.73      1134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(model = mlp,\n",
    "                        epochs = epochs,\n",
    "                        batch_size = batch_size)\n",
    "\n",
    "\n",
    "X_train, X_test = bag_of_words(X_train, X_test, n_gram)\n",
    "\n",
    "\n",
    "# Padding\n",
    "X_train = pad_sequences(X_train, maxlen=max_len, padding='post')\n",
    "X_test = pad_sequences(X_test, maxlen=max_len, padding='post')\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "result = classification_report(y_test, pred)\n",
    "f1 = f1_score(y_test, pred)\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
